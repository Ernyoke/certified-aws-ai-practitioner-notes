# GenAI Capabilities and Challenges

## Capabilities of Generative AI

- Adaptability
- Responsiveness
- Simplicity
- Creativity and exploration
- Data efficiency
- Personalization
- Scalability

## Challenges of Generative AI

- Regulatory violation
- Social risks
- Data security and privacy concerns
- Toxicity
- Hallucinations
- Interpretability
- Nondeterminism

## Toxicity

- Generating content that is offensive, disturbing or inappropriate
- Defining what constitutes "toxicity" can be a challenge
- Boundary between restricting toxic content and censorship is slim
- Also, questions to be considered: what about quotations of someone that can be considered toxic? Should they be included?
- Mitigation:
    - Curate training data by identifying and removing offensive phrases in advance
    - Use guardrail models to detect and filter out unwanted content

## Hallucinations

- Assertions or claims that sound true, but are incorrect
- This is due to the next-word probability sampling employed by LLMs
- This can lead ro content that may not exist, even though the content may seem plausible
- Mitigation:
    - Educate users that content generated by the model must be checked
    - Ensure verification of content with independent sources
    - Mark generated content as unverified to alert users that verification is necessary

## Prompt Misuses

- Poisoning:
    - Intentional introduction of malicious or biased data into the training dataset of a model
    - Leads the model to produce biased, offensive or harmful outputs
- Hijacking and Prompt Injection:
    - Influencing the output by embedding specific instructions within the prompts themselves
    - Hijack to model's behavior and make it produce outputs that align with the attacker's intentions (example: generate misinformation or running malicious code)
- Exposure:
    - The risk of exposing sensitive and confidential information to a model during training or inference
    - The model can then reveal this sensitive data from their training corpus, leading to potential data leaks or privacy violations
- Prompt Leaking:
    - The unintentional disclosure or leakage of the prompts or inputs used within a model
    - It can expose protected data or other data used by the model, such as how the model works
- Jailbreaking:
    - AI models are typically trained with certain ethical and safety constraints in place to prevent misuse or harmful output
    - Jailbreaking is a way to circumvent the constraints and safety measures implemented in a generative model to gain unauthorized access or functionality